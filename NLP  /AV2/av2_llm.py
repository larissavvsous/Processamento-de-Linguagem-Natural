# -*- coding: utf-8 -*-
"""AV2_NLP_LLM_larissa.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n1yEcB7ndae3hV0cys_hidHE1ooVHC0N
"""

from google.colab import drive
drive.mount('/content/drive')

"""## Avaliação 2"""

import pandas as pd
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
from sklearn.metrics import classification_report, confusion_matrix
import torch
import ast

# dados
coment_sem_tratamento = pd.read_csv("/content/drive/MyDrive/content_sem_trat.csv")
print("Dataset bruto:\n")
print(coment_sem_tratamento.head())
coment_pre_process = pd.read_csv("/content/drive/MyDrive/content_lemmatized_stemmed.csv")
print("\nDataset tratado:\n")
print(coment_pre_process.head())

# Distribuição das classes em ambos datasets
print(coment_sem_tratamento['score'].value_counts())
print(coment_pre_process['score'].value_counts())

# Verifica se há GPU disponível e define o dispositivo
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# modelo e o tokenizer
model_name = "distilbert-base-uncased"
tokenizer = DistilBertTokenizer.from_pretrained(model_name)
model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)
model.to(device)

# aplicar LLM no dataset
def classify_with_llm(texts, batch_size=16):
    preds = []
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i + batch_size]
        inputs = tokenizer(batch_texts, return_tensors="pt", padding=True, truncation=True, max_length=512).to(device)
        with torch.no_grad():  # Desativa o cálculo de gradientes
            predictions = model(**inputs)
        preds.extend(torch.argmax(predictions.logits, dim=1).tolist())
    return preds

# Treinamento do modelo
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    save_steps=10_000,
    save_total_limit=2,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=...  # Crie um Dataset PyTorch a partir dos seus dados
)

trainer.train()

# Aplicar a LLM no dataset sem pré-processamento
raw_texts = coment_sem_tratamento['content'].tolist()
y_true = coment_sem_tratamento['score'].tolist()
y_pred_raw = classify_with_llm(raw_texts)

# Métricas de avaliação sem pré-processamento
print("\nResultados sem pré-processamento:")
print(classification_report(y_true, y_pred_raw))
print(confusion_matrix(y_true, y_pred_raw))

# Ajuste no pré-processamento
processed_texts = [ast.literal_eval(tokens) for tokens in coment_pre_process['content_lemmatized'].tolist()]
processed_texts = [' '.join(tokens) for tokens in processed_texts]

y_pred_processed = classify_with_llm(processed_texts)

# Métricas de avaliação com pré-processamento
print("\nResultados com pré-processamento:")
print(classification_report(y_true, y_pred_processed))
print(confusion_matrix(y_true, y_pred_processed))

# Verificando as classes preditas
print("Classes preditas (sem pré-processamento):", set(y_pred_raw))
print("Classes preditas (com pré-processamento):", set(y_pred_processed))

"""### Com treinamento específico"""

import pandas as pd
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments
from sklearn.metrics import classification_report, confusion_matrix
import torch
import ast
from torch.utils.data import Dataset

# Carregando os dados
coment_sem_tratamento = pd.read_csv("/content/drive/MyDrive/content_sem_trat.csv")
print("Dataset bruto:\n")
print(coment_sem_tratamento.head())
coment_pre_process = pd.read_csv("/content/drive/MyDrive/content_lemmatized_stemmed.csv")
print("\nDataset tratado:\n")
print(coment_pre_process.head())

# Distribuição das classes em ambos datasets
print(coment_sem_tratamento['score'].value_counts())
print(coment_pre_process['score'].value_counts())

# Verifica se há GPU disponível e define o dispositivo
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Definindo o modelo e o tokenizer
model_name = "distilbert-base-uncased"
tokenizer = DistilBertTokenizer.from_pretrained(model_name)
model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)
model.to(device)

# Classe para o Dataset
class TextDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]

        # Tokenização
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_length,
            return_token_type_ids=False,
            padding='max_length',
            return_attention_mask=True,
            return_tensors='pt',
            truncation=True
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# Definindo o tamanho máximo dos textos
max_length = 512

# Criando o Dataset para os dados sem pré-processamento
raw_texts = coment_sem_tratamento['content'].tolist()
raw_labels = coment_sem_tratamento['score'].tolist()
train_raw_dataset = TextDataset(raw_texts, raw_labels, tokenizer, max_length)

# Criando o Dataset para os dados pré-processados
processed_texts = [ast.literal_eval(tokens) for tokens in coment_pre_process['content_lemmatized'].tolist()]
processed_texts = [' '.join(tokens) for tokens in processed_texts]
processed_labels = coment_pre_process['score'].tolist()
train_processed_dataset = TextDataset(processed_texts, processed_labels, tokenizer, max_length)

# Configurando os parâmetros de treinamento
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    save_steps=10_000,
    save_total_limit=2,
)

# Treinamento para dados sem pré-processamento
trainer_raw = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_raw_dataset
)
trainer_raw.train()

# Treinamento para dados com pré-processamento
trainer_processed = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_processed_dataset
)
trainer_processed.train()

# Função para aplicar LLM no dataset
def classify_with_llm(texts, batch_size=16):
    preds = []
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i + batch_size]
        inputs = tokenizer(batch_texts, return_tensors="pt", padding=True, truncation=True, max_length=512).to(device)
        with torch.no_grad():  # Desativa o cálculo de gradientes
            predictions = model(**inputs)
        preds.extend(torch.argmax(predictions.logits, dim=1).tolist())
    return preds

# Aplicar a LLM no dataset sem pré-processamento
y_true = coment_sem_tratamento['score'].tolist()
y_pred_raw = classify_with_llm(raw_texts)

# Métricas de avaliação sem pré-processamento
print("\nResultados sem pré-processamento:")
print(classification_report(y_true, y_pred_raw))
print(confusion_matrix(y_true, y_pred_raw))

# Aplicar a LLM no dataset com pré-processamento
y_pred_processed = classify_with_llm(processed_texts)

# Métricas de avaliação com pré-processamento
print("\nResultados com pré-processamento:")
print(classification_report(y_true, y_pred_processed))
print(confusion_matrix(y_true, y_pred_processed))

# Verificando as classes preditas
print("Classes preditas (sem pré-processamento):", set(y_pred_raw))
print("Classes preditas (com pré-processamento):", set(y_pred_processed))

"""### Matrizes de correlação"""

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

# Dados para as matrizes de confusão
matrizes_antes_grid = [
    np.array([[1330, 153], [689, 828]]),  # TF-IDF com pré-processamento
    np.array([[1174, 309], [1196, 321]]),  # TF-IDF sem pré-processamento
    np.array([[958, 525], [967, 550]])  # CountVectorizer com pré-processamento
]

matrizes_apos_grid = [
    np.array([[1332, 205], [561, 902]]),  # TF-IDF com pré-processamento (Alpha: 0.10)
    np.array([[1427, 110], [232, 1231]]),  # TF-IDF com lemmatização (Alpha: 5.00)
    np.array([[999, 538], [216, 1247]])  # CountVectorizer com pré-processamento (Alpha: 5.00)
]

matrizes_av2 = [
    np.array([[4901, 99], [104, 4896]]),  # Sem pré-processamento
    np.array([[4867, 133], [61, 4939]])  # Com pré-processamento
]

# Função para plotar matrizes de confusão lado a lado
def plot_matrices(matrizes, titulos, titulo_principal):
    fig, axes = plt.subplots(1, len(matrizes), figsize=(15, 5))
    fig.suptitle(titulo_principal, fontsize=16)
    for i, (matrix, title) in enumerate(zip(matrizes, titulos)):
        sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[i])
        axes[i].set_title(title)
        axes[i].set_xlabel('Predição')
        axes[i].set_ylabel('Classe Real')
    plt.tight_layout()
    plt.show()

# Plotando as matrizes antes do Grid Search
plot_matrices(matrizes_antes_grid,
              ['TF-IDF com pré-processamento', 'TF-IDF sem pré-processamento', 'CountVectorizer com pré-processamento'],
              'Comparação de Modelos Antes do Grid Search')

# Plotando as matrizes após o Grid Search
plot_matrices(matrizes_apos_grid,
              ['TF-IDF (Alpha: 0.10)', 'TF-IDF com lemmatização (Alpha: 5.00)', 'CountVectorizer (Alpha: 5.00)'],
              'Comparação de Modelos Após Grid Search')

# Plotando as matrizes da AV2
plot_matrices(matrizes_av2,
              ['Sem Pré-processamento', 'Com Pré-processamento'],
              'Resultados da AV2')