# Faça uma implementação para que sua aplicação receba um texto com erros gramaticais em português:
# a) Realize a correção do texto
# b) Tokenize o texto
# c) Remova as stop words
# d) Faça uma tabela mostrando 3 colunas para todos os tokens: token , stemming e lematização

# -*- coding: utf-8 -*-
"""Aula_NLP_21_03.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wCQsYqMaarkMpVphJUvIMEEv_kZj12oz

# Aula de NLP dia: 21/03/2024
"""

import nltk
nltk.download('stopwords')

stopwords = nltk.corpus.stopwords.words('portuguese')

print("Exemplos:", stopwords)

import spacy
from spacy.lang.pt.examples import sentences
#
# alguns exemplos da própria biblioteca

for s in sentences:
  print(s, '\n')

'''!pip install -U spacy
! python -m spacy download pt_core_news_sm'''

nlp = spacy.load("pt_core_news_sm")
doc = nlp(sentences[0])
print(doc.text)

"""### Tokenização"""

for token in doc:
  print(token.text)

"""### Remoção de Stopwords"""

nltk.download('stopwords')
stopwords = nltk.corpus.stopwords.words('portuguese')

remocao_stopwords = ' '.join([token.text for token in doc if token.text.lower() not in stopwords])
print(remocao_stopwords, '\n\n')

"""### Lemmatização"""

for token in doc:
  print(token.text, token.lemma_)

"""## Biblioteca `ENELVO` (normaliza, corrige abreviações, gírias, erros etc)"""

'''!pip install -U enelvo
from enelvo.normaliser import Normaliser
! python -m enelvo download normaliser'''

# instanciando

normalizador = Normaliser(tokenizer='readable', capitalize_inis = True,
                  capitalize_pns = True, capitalize_acs = True,
                  sanitize = True)

"""## Faça uma implementação para que sua aplicação receba um texto com erros gramaticais em português.


"""

mensagem = "O menimo coreu rapidamenti pela rua para pegar o onibuz da escola"

"""### a) Realize a correção do texto

"""

resposta = normalizador.normalise(mensagem)
resposta

# Processar a frase com o modelo do spaCy
doc = nlp(resposta)

nltk.download('punkt')

"""### b) Tokenize o texto"""

from nltk.tokenize import word_tokenize
tokenizado = word_tokenize(resposta.lower())
tokenizado

"""### c) Remova as stop words"""

sem_stopwords = ' '.join([token for token in tokenizado if token not in stopwords])
print(sem_stopwords, '\n\n')

"""### d) Faça uma tabela mostrando 3 colunas para todos os tokens: token, stemming e lematização."""

# stemming
from nltk.stem import RSLPStemmer, WordNetLemmatizer
nltk.download('rslp')  # Stemmer para Português no NLTK
nltk.download('wordnet')
stemmer = RSLPStemmer()
lemmatizer = WordNetLemmatizer()

# lematização
stemmings = [stemmer.stem(token) for token in tokenizado]
print('\nStemmings\n')
stemmings

lematizacoes = [token for token in tokenizado]
print('\nLematizacoes\n')
lematizacoes

import pandas as pd

df = pd.DataFrame({
    'Token': tokenizado,
    'Stemming': stemmings,
    'Lematização': lematizacoes
})

print(df, '\n')

"""### Mostre uma tabela com resultados da pesquisa sobre os significados das siglas pos tagging que foram calculadas pela spaCy."""

pos_tags = []
significados = []
palavras = []

for token in doc:
    # Verificar a POS tag de cada token e adicionar à lista correspondente
    if token.pos_ == 'PROPN':
        pos_tags.append('propn')
        significados.append('substantivo próprio')
        palavras.append(token.text)
    elif token.pos_ == 'AUX':
        pos_tags.append('aux')
        significados.append('verbo auxiliar')
        palavras.append(token.text)
    elif token.pos_ == 'VERB':
        pos_tags.append('verb')
        significados.append('verbo')
        palavras.append(token.text)
    elif token.pos_ == 'DET':
        pos_tags.append('det')
        significados.append('determinante')
        palavras.append(token.text)
    elif token.pos_ == 'NOUN':
        pos_tags.append('noun')
        significados.append('substantivo')
        palavras.append(token.text)
    elif token.pos_ == 'ADV':
        pos_tags.append('adv')
        significados.append('advérbio')
        palavras.append(token.text)
    elif token.pos_ == 'ADP':
        pos_tags.append('adp')
        significados.append('preposição ou subordinativa')
        palavras.append(token.text)
    elif token.pos_ == 'NUM':
        pos_tags.append('num')
        significados.append('numeral')
        palavras.append(token.text)
    elif token.pos_ == 'SYM':
        pos_tags.append('sym')
        significados.append('símbolo')
        palavras.append(token.text)
    else:
        # Para outras POS tags, adicionamos valores vazios
        pos_tags.append('')
        significados.append('')
        palavras.append('')

df = pd.DataFrame({'POS Tag': pos_tags, 'Significado': significados, 'Palavra': palavras})
df